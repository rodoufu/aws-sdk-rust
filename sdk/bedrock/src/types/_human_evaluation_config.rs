// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Specifies the custom metrics, how tasks will be rated, the flow definition ARN, and your custom prompt datasets. Model evaluation jobs use human workers <i>only</i> support the use of custom prompt datasets. To learn more about custom prompt datasets and the required format, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-custom.html">Custom prompt datasets</a>.</p>
/// <p>When you create custom metrics in <code>HumanEvaluationCustomMetric</code> you must specify the metric's <code>name</code>. The list of <code>names</code> specified in the <code>HumanEvaluationCustomMetric</code> array, must match the <code>metricNames</code> array of strings specified in <code>EvaluationDatasetMetricConfig</code>. For example, if in the <code>HumanEvaluationCustomMetric</code> array your specified the names <code>"accuracy", "toxicity", "readability"</code> as custom metrics <i>then</i> the <code>metricNames</code> array would need to look like the following <code>["accuracy", "toxicity", "readability"]</code> in <code>EvaluationDatasetMetricConfig</code>.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct HumanEvaluationConfig {
    /// <p>The parameters of the human workflow.</p>
    pub human_workflow_config: ::std::option::Option<crate::types::HumanWorkflowConfig>,
    /// <p>A <code>HumanEvaluationCustomMetric</code> object. It contains the names the metrics, how the metrics are to be evaluated, an optional description.</p>
    pub custom_metrics: ::std::option::Option<::std::vec::Vec<crate::types::HumanEvaluationCustomMetric>>,
    /// <p>Use to specify the metrics, task, and prompt dataset to be used in your model evaluation job.</p>
    pub dataset_metric_configs: ::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>,
}
impl HumanEvaluationConfig {
    /// <p>The parameters of the human workflow.</p>
    pub fn human_workflow_config(&self) -> ::std::option::Option<&crate::types::HumanWorkflowConfig> {
        self.human_workflow_config.as_ref()
    }
    /// <p>A <code>HumanEvaluationCustomMetric</code> object. It contains the names the metrics, how the metrics are to be evaluated, an optional description.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.custom_metrics.is_none()`.
    pub fn custom_metrics(&self) -> &[crate::types::HumanEvaluationCustomMetric] {
        self.custom_metrics.as_deref().unwrap_or_default()
    }
    /// <p>Use to specify the metrics, task, and prompt dataset to be used in your model evaluation job.</p>
    pub fn dataset_metric_configs(&self) -> &[crate::types::EvaluationDatasetMetricConfig] {
        use std::ops::Deref;
        self.dataset_metric_configs.deref()
    }
}
impl HumanEvaluationConfig {
    /// Creates a new builder-style object to manufacture [`HumanEvaluationConfig`](crate::types::HumanEvaluationConfig).
    pub fn builder() -> crate::types::builders::HumanEvaluationConfigBuilder {
        crate::types::builders::HumanEvaluationConfigBuilder::default()
    }
}

/// A builder for [`HumanEvaluationConfig`](crate::types::HumanEvaluationConfig).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct HumanEvaluationConfigBuilder {
    pub(crate) human_workflow_config: ::std::option::Option<crate::types::HumanWorkflowConfig>,
    pub(crate) custom_metrics: ::std::option::Option<::std::vec::Vec<crate::types::HumanEvaluationCustomMetric>>,
    pub(crate) dataset_metric_configs: ::std::option::Option<::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>>,
}
impl HumanEvaluationConfigBuilder {
    /// <p>The parameters of the human workflow.</p>
    pub fn human_workflow_config(mut self, input: crate::types::HumanWorkflowConfig) -> Self {
        self.human_workflow_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>The parameters of the human workflow.</p>
    pub fn set_human_workflow_config(mut self, input: ::std::option::Option<crate::types::HumanWorkflowConfig>) -> Self {
        self.human_workflow_config = input;
        self
    }
    /// <p>The parameters of the human workflow.</p>
    pub fn get_human_workflow_config(&self) -> &::std::option::Option<crate::types::HumanWorkflowConfig> {
        &self.human_workflow_config
    }
    /// Appends an item to `custom_metrics`.
    ///
    /// To override the contents of this collection use [`set_custom_metrics`](Self::set_custom_metrics).
    ///
    /// <p>A <code>HumanEvaluationCustomMetric</code> object. It contains the names the metrics, how the metrics are to be evaluated, an optional description.</p>
    pub fn custom_metrics(mut self, input: crate::types::HumanEvaluationCustomMetric) -> Self {
        let mut v = self.custom_metrics.unwrap_or_default();
        v.push(input);
        self.custom_metrics = ::std::option::Option::Some(v);
        self
    }
    /// <p>A <code>HumanEvaluationCustomMetric</code> object. It contains the names the metrics, how the metrics are to be evaluated, an optional description.</p>
    pub fn set_custom_metrics(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::HumanEvaluationCustomMetric>>) -> Self {
        self.custom_metrics = input;
        self
    }
    /// <p>A <code>HumanEvaluationCustomMetric</code> object. It contains the names the metrics, how the metrics are to be evaluated, an optional description.</p>
    pub fn get_custom_metrics(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::HumanEvaluationCustomMetric>> {
        &self.custom_metrics
    }
    /// Appends an item to `dataset_metric_configs`.
    ///
    /// To override the contents of this collection use [`set_dataset_metric_configs`](Self::set_dataset_metric_configs).
    ///
    /// <p>Use to specify the metrics, task, and prompt dataset to be used in your model evaluation job.</p>
    pub fn dataset_metric_configs(mut self, input: crate::types::EvaluationDatasetMetricConfig) -> Self {
        let mut v = self.dataset_metric_configs.unwrap_or_default();
        v.push(input);
        self.dataset_metric_configs = ::std::option::Option::Some(v);
        self
    }
    /// <p>Use to specify the metrics, task, and prompt dataset to be used in your model evaluation job.</p>
    pub fn set_dataset_metric_configs(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>>) -> Self {
        self.dataset_metric_configs = input;
        self
    }
    /// <p>Use to specify the metrics, task, and prompt dataset to be used in your model evaluation job.</p>
    pub fn get_dataset_metric_configs(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>> {
        &self.dataset_metric_configs
    }
    /// Consumes the builder and constructs a [`HumanEvaluationConfig`](crate::types::HumanEvaluationConfig).
    /// This method will fail if any of the following fields are not set:
    /// - [`dataset_metric_configs`](crate::types::builders::HumanEvaluationConfigBuilder::dataset_metric_configs)
    pub fn build(self) -> ::std::result::Result<crate::types::HumanEvaluationConfig, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::types::HumanEvaluationConfig {
            human_workflow_config: self.human_workflow_config,
            custom_metrics: self.custom_metrics,
            dataset_metric_configs: self.dataset_metric_configs.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "dataset_metric_configs",
                    "dataset_metric_configs was not specified but it is required when building HumanEvaluationConfig",
                )
            })?,
        })
    }
}
